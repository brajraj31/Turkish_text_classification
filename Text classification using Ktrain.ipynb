{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('turkish_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>3 milyon ile ön seçim vaadi mhp nin 10 olağan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>mesut_yılmaz yüce_divan da ceza alabilirdi pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text\n",
       "0  siyaset    3 milyon ile ön seçim vaadi mhp nin 10 olağan...\n",
       "1  siyaset    mesut_yılmaz yüce_divan da ceza alabilirdi pr..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records present in dataset: 4900\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of records present in dataset: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Dataset consist of news with different category like 'dunya'(word), siyaset(Politics), ekonomi(economics) etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total different category in Datatset:\n",
      "['siyaset ' 'dunya ' 'ekonomi ' 'kultur ' 'saglik ' 'spor ' 'teknoloji ']\n"
     ]
    }
   ],
   "source": [
    "print(\"Total different category in Datatset:\")\n",
    "print(df['category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment belwo line to install ktrain \n",
    "#!pip install ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\BRAJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\BRAJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import required packaged\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "stemmer = TurkishStemmer()\n",
    "\n",
    "\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_cleaning(df_data):\n",
    "    \"\"\"This function helps to remove row with missing value or if there is any dupicate records\"\"\"\n",
    "    df_data = df_data.dropna()\n",
    "    df_data = df_data.drop_duplicates()\n",
    "    df_data = df_data.reset_index(drop=True)\n",
    "    return df_data\n",
    "\n",
    "def text_cleaning(text):\n",
    "    \"\"\"This function helps to clean a text after removing stop words, short words, special character,\n",
    "    any link present and use stemmer to provide near to root word\"\"\"\n",
    "    stop = set(stopwords.words(\"turkish\"))\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^abcçdefgğhıijklmnoöprsştuüvyzmi̇z]',' ', text)\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = \" \".join([stemmer.stem(word) for word in text.split() if (word not in stop) and len(word)>1])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4540, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>3 milyon ile ön seçim vaadi mhp nin 10 olağan...</td>\n",
       "      <td>milyon ön seç vaadi mhp nin olağan büyük kurul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>mesut_yılmaz yüce_divan da ceza alabilirdi pr...</td>\n",
       "      <td>mesut yılmaz yüç divan ceza alabilir prof dr s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text  \\\n",
       "0  siyaset    3 milyon ile ön seçim vaadi mhp nin 10 olağan...   \n",
       "1  siyaset    mesut_yılmaz yüce_divan da ceza alabilirdi pr...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  milyon ön seç vaadi mhp nin olağan büyük kurul...  \n",
       "1  mesut yılmaz yüç divan ceza alabilir prof dr s...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['cleaned_text'] = dataset['text'].apply(lambda x:text_cleaning(x))\n",
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset into Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(dataset['cleaned_text']), np.array(dataset['category']),\n",
    "                                                   test_size = 0.20, random_state=42, stratify = dataset['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electra model ref: https://huggingface.co/dbmdz/electra-base-turkish-cased-discriminator\n",
    "i am going to use ELECTRA instead of BERT as it peformed beter with high GLEU score in different research. Electra perfomance is better when you have computing resource limitation. It takes less training time comparing to BERT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: tr\n",
      "train sequence lengths:\n",
      "\tmean : 233\n",
      "\t95percentile : 559\n",
      "\t99percentile : 896\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: tr\n",
      "test sequence lengths:\n",
      "\tmean : 240\n",
      "\t95percentile : 601\n",
      "\t99percentile : 846\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "MODEL_NAME = 'dbmdz/electra-base-turkish-cased-discriminator'\n",
    "t= text.Transformer(MODEL_NAME, maxlen=500, classes= dataset['category'].unique())\n",
    "train = t.preprocess_train(X_train, y_train)\n",
    "val = t.preprocess_test(X_test, y_test)\n",
    "model = t.get_classifier()\n",
    "leaner = ktrain.get_learner(model, train_data = train, val_data = val, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leaner.lr_find() #to find good leaning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaner.fit_onecycle(5e-5, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dl",
   "language": "python",
   "name": "env_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
